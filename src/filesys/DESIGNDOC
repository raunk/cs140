      	      +-------------------------+
		     |		CS 140	       |
		     | PROJECT 4: FILE SYSTEMS |
		     |	   DESIGN DOCUMENT     |
		     +-------------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Eric Conner <econner@stanford.edu>
Jeremy Keeshin <jkeeshin@stanford.edu>
Charlie Fang <charlief@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

We have the VM functionality turned on in our submission.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

		     INDEXED AND EXTENSIBLE FILES
		     ============================

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* In-memory inode. */
struct inode 
  {
    struct list_elem elem;   /* Element in inode list. */
    block_sector_t sector;   /* Sector number of disk location. */
    int open_cnt;            /* Number of openers. */
    bool removed;            /* True if deleted, false otherwise. */
    int deny_write_cnt;      /* 0: writes ok, >0: deny writes. */
    int read_limit;          /* Reads not allowed passed here (used for
                                           concurrent access) */
    struct lock extending_file_lock;  /* Prevents two threads from extending a file at the same time */
  };

 /* On-disk inode.
    Must be exactly BLOCK_SECTOR_SIZE bytes long. */
 struct inode_disk
   {
     block_sector_t start;               /* First data sector. */
     off_t length;                       /* File size in bytes. */
     unsigned magic;                     /* Magic number. */

     /* The start of the multilevel index used by each inode */
      block_sector_t index[INODE_INDEX_COUNT];
     bool is_dir; 	/* Whether this inode represents a directory */
     uint32_t unused[110];               /* Not used. */
   };
 
 /* A struct which represents pointers to disk blocks for single
  * and doubly indirect blocks for the multilevel index */
 struct indirect_block
   {
     uint32_t pointers[NUM_BLOCK_POINTERS];
   };


// In directory.c

 11 /* A directory. */                                                                  
 12 struct dir                                                                          
 13   {                                                                                 
 14     struct inode *inode;                /* Backing store. */                        
 15     off_t pos;                          /* Current position. */                     
 16     struct lock lock;                      /* Lock to prevent concurrent            
 17                                 operations on same directory */                     
 18   };  


>> A2: What is the maximum size of a file supported by your inode
>> structure?  Show your work.

We use the multilevel index scheme described in class with 14
pointers per inode meta-data entry. We have 12 direct blocks,
one indirect block pointer, and one doubly indirect pointer.

Each indirect block can hold 128 pointers which is 512/4 bytes,
since each block_sector_t is 32 bits = 4 bytes, and each sector
is 512 bytes.

Therefore, the max file size =

(12 + 128 + 128*128) * 512 = 8460288 bytes


---- SYNCHRONIZATION ----

>> A3: Explain how your code avoids a race if two processes attempt to
>> extend a file at the same time.

When we check the length of a file when we are writing to an inode, we 
return whether or not we are extending the file. If we are extending the 
file, we acquire the lock within the inode so that we are the only person 
writing at this point. This lock is only needed if you are extending the file, 
so it will block other processes also trying to extend the file. Other reads 
and writes to this file will be permitted as they should.

>> A4: Suppose processes A and B both have file F open, both
>> positioned at end-of-file.  If A reads and B writes F at the same
>> time, A may read all, part, or none of what B writes.  However, A
>> may not read data other than what B writes, e.g. if B writes
>> nonzero data, A is not allowed to see all zeros.  Explain how your
>> code avoids this race.

In our in memory inode struct, we store a variable called read_limit. Initially 
the read_limit is set to -1, meaning that there is no read limit. However when 
we are in inode_write_at, we call a method called “check_length.” If we are ever 
writing past the end of the file and extending it, we say that the read_limit is 
the previous length. This means that we are extending the file so any readers at 
this point in time should not see this extension and should only be able to read 
up to the read limit.

In inode_read_at, if we find that there is a read limit, we change the length 
of the inode that we can read to match this limit.

Once inode_write_at completes, it says again that there is no read limit. 

>> A5: Explain how your synchronization design provides "fairness".
>> File access is "fair" if readers cannot indefinitely block writers
>> or vice versa.  That is, many processes reading from a file cannot
>> prevent forever another process from writing the file, and many
>> processes writing to a file cannot prevent another process forever
>> from reading the file.

File reading and file writing is not blocking in our implementation, unless two 
writers are trying to write past the end of the file. inode_read_at just reads 
from the buffer cache, which is not blocking unless there is i/o involved. 
However, if a sector is brought into memory, then it will be at the front of 
the buffer cache, and not require i/o immediately. Likewise, with inode_write_at, 
since it operates through the buffer cache, multiple threads can write to it, and 
even while it is being written, others can read from it, since this memory is 
available through the buffer cache. 

Continually reading or continually writing will also not starve other threads 
because since we operate always through the cache, when these sectors are 
requested, they are brought in from disk if they are not available. They are 
both going through the cache, and although one thread may cause another’s sectors 
to be evicted, they do not have a monopoly over the cache. Locking only occurs at 
the level of an individual sector and since the cache has 64 available sectors the 
reader and writer each can always get an entry in the cache for the block they 
want to read or write. 

---- RATIONALE ----

>> A6: Is your inode structure a multilevel index?  If so, why did you
>> choose this particular combination of direct, indirect, and doubly
>> indirect blocks?  If not, why did you choose an alternative inode
>> structure, and what advantages and disadvantages does your
>> structure have, compared to a multilevel index?

We chose the multilevel index used in the 4.3 BSD Unix scheme
because it seemed like a tested design, and also supports large
enough files. Our design has all the advantages of a multilevel index, 
including that we are able to support large and extensible files, but
those sectors are able to be allocated lazily when they are needed.

			    SUBDIRECTORIES
			    ==============

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

In struct file_descriptor_elem:
  struct dir *dir;  /* If this open file is a directory we need a
                       handle to its dir pointer */
In struct thread:
  struct dir* working_directory; /* Pointer to current working
                                    dir of thread */


---- ALGORITHMS ----

>> B2: Describe your code for traversing a user-specified path.  How
>> do traversals of absolute and relative paths differ?

We wrote a wrapper function that takes a user-specified path, determines 
whether the path is relative or absolute, and then makes an initial call 
to a recursive function that parses the path.  The parse is done by each 
time extracting the next component of the path name, reading its directory 
entry in the current directory, and then recursing on the rest of the path 
until we find the entry of interest.  The only difference between traversing 
an absolute and relative path is the initial directory.  For a relative path 
we start from the current working directory and an absolute path starts 
from the root directory.

---- SYNCHRONIZATION ----

>> B4: How do you prevent races on directory entries?  For example,
>> only one of two simultaneous attempts to remove a single file
>> should succeed, as should only one of two simultaneous attempts to
>> create a file with the same name, and so on.

Each directory has a lock, and on calls that modify the directory, you 
must acquire the lock to make these modifications. This includes dir_add, 
dir_remove, and readdir, since this modifies the offset into the directory.

>> B5: Does your implementation allow a directory to be removed if it
>> is open by a process or if it is in use as a process's current
>> working directory?  If so, what happens to that process's future
>> file system operations?  If not, how do you prevent it?

No, we do not allow directories to be removed if they are the current 
working directory or if open.  We prevent it by explicitly checking 
the list of open directories and the current working directory for the 
inode the user wants to remove.  If it is open we fail.

---- RATIONALE ----

>> B6: Explain why you chose to represent the current directory of a
>> process the way you did.

We chose to store a pointer to the current directory of a process in the 
thread struct.  This made the most sense accessing the directory and made 
it easy to manipulate the current directory in a chdir call.  For example, 
if the process changes directories then we need to close the directory it 
was just in and open the new one.  This directory pointer allowed us to 
keep track of just one instance of the struct dir pointer instead of 
having multiple calls to lookup the directory only to close it.  We 
thought about storing just the inumber or inode of the directory, but 
this required us to actually open the directory in order to close it.

			     BUFFER CACHE
			     ============

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

Represents one element in the cache.

struct cache_elem {
	block_sector_t sector; 		// The sector number
	bool is_dirty;				// Whether or not it is dirty
	int num_operations;			// Pending operations, for synchronization
	struct list_elem list_elem; /* List element for cache*/
	struct hash_elem hash_elem; /* Hash element for cache*/
	char data[512]; 			/* Cache data */
};
static struct lock done_lock;	// For properly synchronizing
								// cache_done with read ahead thread 
static int cache_stop = 0;	// Flag when entering cache_done
 
static struct list cache_list;	// Linked list to find LRU element
static struct hash cache_hash;	// Hash to find cache elements
 
static struct lock cache_lock; // Global cache lock
static struct condition io_finished; // Condition when io operation completes

// Condition to wait on when evicting to make sure no operations
// are pending to this block
static struct condition operations_finished;

static struct list sectors_under_io; // List of sectors currently under io
static block_sector_t sector_max; // Largest sector in the disk

struct sector_under_io {
	block_sector_t sector;
	// List element to store all sectors that are under io
	struct list_elem elem;
};

// Stats to analyze cache
static int cache_hits;
static int cache_misses;

// Element to always keep free map meta-data around
struct cache_elem free_map_cache; 


// Threads for running read ahead and write behind
static struct thread *read_ahead_thread;
static struct thread *write_behind_thread;

static struct list read_ahead_queue; /* List of sectors to read from disk */
static struct lock read_ahead_lock;	 /* Protects read ahead list */
static struct condition do_read_ahead; /* Signals when there's something to
										  read ahead */

struct read_ahead_sector {
  block_sector_t sector;
  struct list_elem elem;
};


---- ALGORITHMS ----

>> C2: Describe how your cache replacement algorithm chooses a cache
>> block to evict.

Our cache implementation is a LRU cache with a hash map and a linked
list implementation. The hash map allows constant time access to the elements
to determine if they are in the cache, and the linked list lets us easily find
which was the least recently used element to evict.

Each time we look up a sector in the cache, we bring it to the front 
of the list. Therefore, the element that should be removed is the one 
at the back of the list.

We choose the element from the back of the list. If the sector is already 
under I/O, we wait for it to complete.  Then we wait until there are no 
operations  on this sector before writing it back to disk.

>> C3: Describe your implementation of write-behind.

We spawn a thread that in a while loop calls timer sleep for 
CACHE_FLUSH_WAIT_MS milliseconds, and then flushes the cache.

To flush the cache, we iterate through all of the cache elements, and 
if they are dirty and not currently under I/O, we then write it out to disk. We 
release the lock right before writing to disk and acquire right after so that it 
is not blocking. Note that if the block is dirty and under I/O, it must be in 
the process of eviction; therefore, it is being written to disk through eviction 
already and does not need to be flushed.

We treat block flush as an operation, not an I/O action. Because we are not 
evicting the block, other processes should be allowed to read from/write to 
the block while it is being written to disk. Like other operations, this 
should block I/O actions.

>> C4: Describe your implementation of read-ahead.

We keep a queue of sectors to be read-ahead. In inode_read_at, we call 
handle_read_ahead, which finds the next sector that would be read for 
this read request, and adds an element to the queue of sectors to be
read-ahead. If the list was empty, we alert the read-ahead thread that 
it should begin to read ahead. 

We spawn a thread that in a while loop checks this queue of sectors, 
and if it isn’t empty, reads that sector into the cache. If the list is 
empty, the thread waits for a condition to be signaled that the list 
is no longer empty so that it is non-blocking. 

We also have synchronization so that the read-ahead thread does not 
continue once cache_done has been called and the cache elements are 
about to be freed. 

---- SYNCHRONIZATION ----

>> C5: When one process is actively reading or writing data in a
>> buffer cache block, how are other processes prevented from evicting
>> that block?

Each buffer cache block struct has a ‘num_operations’ field that indicates the 
number of processes currently reading from or writing to that block. Any time 
data needs to be read from or written to a block, that block’s num_operations 
count is incremented, and after the reads and writes are finished, the count 
is decremented. After the count is decremented, if the count is equal, then the 
reading or writing process broadcasts to all processes waiting on the 
‘operations_finished’ condition variable. Any process that wants to evict a block 
must wait on the condition variable ‘operations_finished’ within a while loop: 
while (c->num_operations > 0). In other words, the evicting process waits until no other 
processes are reading from or writing to the desired block. 

Note that there is one global condition variable ‘operations_finished’ for all 
buffer cache blocks, so anytime a block’s ‘num_operations’ field reaches 0, all 
waiting evicting processes, regardless or which block they are waiting on, are 
signaled. This causes some inefficiencies, but it is not nearly as bad as busy-waiting. 

>> C6: During the eviction of a block from the cache, how are other
>> processes prevented from attempting to access the block?

We maintain a list of sectors that are currently under I/O (i.e. they 
are either in the process of being read from disk into the cache OR 
evicted and written to disk). Whenever a process begins evicting a 
block, the sector number is added to the list. Any processes that 
try to read from or write to any sector in the list must wait on the 
condition variable ‘io_finished’ within a while loop: while 
(is_sector_under_io(c->sector)). The is_sector_under_io() function 
returns true if the given sector is in the list of sectors under I/O, 
false otherwise. Once the evicting process finishes evicting the block, it 
removes the sector from the list and broadcasts to all processes waiting 
on ‘io_finished’. 

As in the case described in C5, there may be some inefficiencies 
due to there being only one global condition variable. 


---- RATIONALE ----

>> C7: Describe a file workload likely to benefit from buffer caching,
>> and workloads likely to benefit from read-ahead and write-behind.

buffer cache
A workload that requires long seeks would benefit from buffer caching.  
This would be a workload where the files accessed frequently are 
spread out across the disk and would take a very long time to 
seek to for a sequential access.

read-ahead
Any kind of sequential read of a file where the file’s blocks 
are requested in order.

write-behind
write-behind improves the stability of the disk so any workload 
that is sensitive to data loss would benefit from write-behind.  Write 
behind tries to make sure that the data on disk reflects what has 
actually been written in case of mishaps.

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students in future quarters?

>> Any other comments?